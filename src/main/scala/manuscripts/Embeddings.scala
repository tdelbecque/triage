package com.sodad.els.triage.manuscripts

import scala.collection.JavaConversions._
import java.util.Properties
import edu.stanford.nlp.pipeline._
import edu.stanford.nlp.ling.CoreAnnotations._
import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql._
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.sql.types.StringType
import com.sodad.els.triage.config._

/** This trait is used to transform tokens during tokenization */
trait Transformer extends java.io.Serializable {
  def transform (term: String) : String
}

class LowerCaseTransformer extends Transformer {
  def transform (term: String) = term toLowerCase
}

/** Records of datasets holding sequences of tokens, associated with an id.
  * It is typically generated with a call to [[EmbeddingApp.tokenizePerSentence]] */
case class TokenizedRecord (
  id: String,
  terms: Seq[String])

/** a manuscript (identified by id), transformed as a weighed BoW. 
  * It is typically generated by a call to createWeighted */
case class WeightedTermRecord (
    id: String,
    weights: Map[String, Double])

/** a word embedding associates a vector (here supported by a 
  * sequence) to a term (the word) */
case class WordEmbeddingRecord (word: String, embedding: Seq[Float])

/** an embedding associates a vector (here supported by a sequence)
  * to an entity identified by id.
  * Currently all computed embeddings, with the exception of 
  * word embeddings, are supported by a  EmbeddingRecord[Double] record */
case class EmbeddingRecord[T] (id: String, embedding: Seq[T])
case class EmbeddingRecordWithYear[T] (id: String, embedding: Seq[T], year: Int)

/** Record to support One Hot Encoding dataset for training classifiers */
case class OHETrainingDataRecord (id: String, ohe: Array[Int], features: Array[Double])
 
/** Classification performance estimates for an "One Hot Encoded" target. */
case class OHEClassificationModelPerformancesRecord (
  /** the label of the class */
  label: String,
  /** number of times this class rightly was predicted */
  fitCount: Long,
  /** number of cases in the test set */
  totalCount: Long,
  /** fitcount / totalCount */
  accuracy: Double
)

/** Structure of a logistic regression modeling */
case class LRModelTransformRecord (
    label: Double,
    features: Vector,
    rawPrediction: Vector,
    probability: Vector,
    prediction: Double
)

/** Structure to support estimation of performances of classification
  * models */
case class ModelEvaluationRecord (
    numberOfCase: Long,
    numberOfPositive: Long,
    numberOfPredictedPositive: Long,
    numberOfTruePositive: Long,
    numberOfTrueNegative: Long
) {
  val numberOfNegative = numberOfCase - numberOfPositive
  val numberOfPredictedNegative = numberOfCase - numberOfPredictedPositive
  val numberOfFalsePositive = numberOfPredictedPositive - numberOfTruePositive
  val numberOfFalseNegative = numberOfPredictedNegative - numberOfTrueNegative
  lazy val accuracy = (numberOfTruePositive + numberOfTrueNegative).toDouble / numberOfCase
  lazy val prevalence = numberOfPositive.toDouble / numberOfCase
  lazy val positivePredictiveValue = numberOfTruePositive.toDouble / numberOfPositive
  lazy val negativePredictiveValue = numberOfTrueNegative.toDouble / numberOfNegative

  def +(m: ModelEvaluationRecord) = ModelEvaluationRecord (
    numberOfCase = numberOfCase + m.numberOfCase,
    numberOfPositive = numberOfPositive + m.numberOfPositive,
    numberOfPredictedPositive = numberOfPredictedPositive + m.numberOfPredictedPositive,
    numberOfTruePositive = numberOfTruePositive + m.numberOfTruePositive,
    numberOfTrueNegative = numberOfTrueNegative + m.numberOfTrueNegative )
}

object ModelEvaluationRecord {
    def empty = ModelEvaluationRecord (0, 0, 0, 0, 0)
}

/** Pattern replacement utilities 
  * it replaces numerical expressions with tags 
  * but should take care of other strings such as chemicals, genes, etc */
package object replacement {
  /** Replace numerical expressions.
    * - +1.3 % => sodadpct
    * - = 1.3 => sodadnumexpr
    * - 1.3 = => sodadnumexpr
    * - 1.3 => sodadnum       */
  val defaultReplacementPatterns = {
    val floatStr = """(?:\+/-|\+-|\+|-)?\s*\d+(?:\.\d+)*"""
    List (
      s"$floatStr\\s*%".r -> " sodadpct ",
      s"[<>=]+\\s*$floatStr".r -> " sodadnumexpr ",
      s"$floatStr\\s*[<>=]+".r -> " sodadnumexpr ",
      s"$floatStr".r -> " sodadnum ",
      """\(.*?\)""".r -> " ",
      """\s+""".r -> " ")
  }

  /** Apply a sequence of pattern replacements to a record instantiated as an (id, text) pair.
    * @param r: an `(id, text)` pair
    * @param patterns: the set of replacements to apply to `text`
    * @return: an `(id, repl_text)` pair where `repl_text` is the result of the replacements applied to `text` */
  def replacePatternsInRecord (
    r: (String, String),
    patterns: Seq[(scala.util.matching.Regex, String)] = defaultReplacementPatterns
    ) : (String, String) = ( 
      r._1,
      patterns.foldLeft (r._2) { case (a, (e, r)) => e replaceAllIn (a, r) } )

  /** Replace patterns to all records of a dataset.
    * @param data: the data to process
    * @param maybeSavePath: where to save to new dataset */
  def replacePatterns(data: Dataset[(String, String)],
    patterns: Seq[(scala.util.matching.Regex, String)] = defaultReplacementPatterns,
    maybeSavePath: Option[String] = None)
    (implicit session: SparkSession): Dataset[(String, String)] = {
    import session.implicits._
    val replacement = data map (replacePatternsInRecord(_, patterns))
    maybeSavePath foreach { replacement.write.option ("path", _) save }
    replacement
  }
  
}

/** Some constants and utility functions, related to manuscripts embedding, 
  * but that does not need a spark session */
object EmbeddingApp {
  /** Apply tokenization to a sequence of texts.
    * @param texts  what to tokenize
    * @param maybeTransformer  the transformer to apply to each tokens, once the tokenization is done
    * @return a sequence of sequences of token, one sequence per text in the `texts` parameter */
  def tokenizeTextPerSentence (texts: Seq[String], maybeTransformer: Option[Transformer] = None) = {
    val transform: String => String =
      maybeTransformer.
        map { _.transform _ }.
        getOrElse { x => x }
    val props = new Properties ()
    props put ("annotators", "tokenize, ssplit")
    val pipeline = new StanfordCoreNLP (props)
    texts.map { t =>
      val ann = new Annotation (t)
      pipeline annotate ann
        (for (s <- ann get classOf[SentencesAnnotation]) // split sentences
        yield (
          (for (t <- s.get (classOf[TokensAnnotation]).map { x => 
            transform (x.originalText()) }) yield t) toList
        )).
        toList.
        flatMap {x => x} // reduces a 2 levels list to a 1 level one.
    }
  }

}

/** All manuscripts embedding utilities, when a spark session is needed.
  * Per default, the manuscripts dataset is the one computed from [[ManuscriptsApp.doitExtractManuscriptsContent]]
  *  and is stored to [[ManuscriptsApp.path_manuscripts]] */ 
class EmbeddingApp (val config: EmbeddingAppConfig) (implicit val session: SparkSession) {
  type ManuscriptId = String
  type Term = String
  type ManuscriptEmbeddingRecord = EmbeddingRecord[Double]

  import EmbeddingApp._
  import session.implicits._

  val pr = session.read.parquet _
  val sc = session.sparkContext

  val pc = config.persist

  val numberOfAreas = 27

  /** the whole set of "complete" manuscripts */
  lazy val manuscripts = 
    session.read.parquet (pc.getPathManuscripts).as[ManuscriptsContentRecord]
  /** the manuscripts restricted to the titles */
  lazy val titles = manuscripts.select ("title").map (_.getString (0))
  /** the manuscripts restricted to the abstracts */
  lazy val abstracts = manuscripts.select ("abstr").map (_.getString (0))

  lazy val manuscriptsEmbeddings = 
    session.read.parquet (pc getPathManuscriptsEmbedding).
      toDF ("id", "embedding").
      as[ManuscriptEmbeddingRecord]

  /** A dataframe (fields = `eid`, `area`) holding the subject areas for each paper.
    * One area per record. Computation is performed on [[manuscripts]] */
  lazy val flattenSubjAreas =
    manuscripts.select ("eid", "subjareas").flatMap { x =>
      val eid = x getString 0
      val areas: Seq[String] = x getSeq[String] 1
      areas.map { (eid, _) }
    }.
      toDF ("eid", "area")
  
  /** A dataframe (fields = `area`, `freq`, `pct`) for counting frequencies of areas.  
    * Computation is done on [[manuscripts]] */
  lazy val areaFrequencies =
    flattenSubjAreas.
      groupBy ("area").count.
      select ($"area", $"count" as "freq", ($"count" * 100.0 / manuscripts.count) as "pct").
      orderBy ($"count" desc)

  /** Given a dataframe of `(String, String)` where `_1` is an id, and `_2` is the text to
    *  tokenize, creates a record for each sentence in `_2`.
    *  @param data what to tokenize
    *  @param maybeTransformer if not `None`, a tranformer that will be applied to each token
    *  @param maybeSavePath where to write the new dataset
    *  @return tokenized dataset. */
  def tokenizePerSentence (
    data: Dataset[(String, String)],
    maybeTransformer: Option[Transformer] = None,
    maybeSavePath: Option[String]
  ) : Dataset[TokenizedRecord] = {
    val transform : String => String = maybeTransformer match {
      case Some (s) => s.transform _
      case _ => x => x
    }

    val tokenized = data mapPartitions { it =>
      val props = new Properties ()
      props put ("annotators", "tokenize, ssplit")
      val pipeline = new StanfordCoreNLP (props)
      it flatMap { case (id, payload) =>
        val ann = new Annotation (payload)
        pipeline annotate ann
        (for (s <- ann get classOf[SentencesAnnotation]) yield (
          for (t <- s.get (classOf[TokensAnnotation]).map { x =>
            transform (x.originalText ())})
          yield t).toList).map { TokenizedRecord (id, _) }.toList
      }
    }
    maybeSavePath foreach ( tokenized.write.mode ("overwrite").option ("path", _).save )
    tokenized
  }

  /** Tokenize abstracts (after pattern replacement and conversion to lower case) 
    *  and save the result. */
  def saveTokenizedAbstracts (data: DataFrame, pathToSave: String) = {
    val abstracts = data.select ("eid", "abstr").as[(String, String)]
//      map {x => (x getString 0, x getString 1)}
    val abstractsReplace = replacement.replacePatterns (abstracts).map {x => (x._1, x._2.toLowerCase)}
    val tokens = tokenizePerSentence (abstractsReplace, None, None).map (_ terms)
    tokens.write.option ("path", pathToSave) save
  }

  /** Helper function. Given a dataset of sequences of tokens, and a ``per sequence`` aggregating function
    * computes the frequencies of each token in the dataset.
    * Definition of this frequency depends on the aggregating function.
    * @param data the dataset where to estimate the frequencies
    * @param seqop the aggregating function to apply to each individual sequence
    * @return a map `token => frequency` */
  private def extractVocabularyHelper (
    data: Dataset[Seq[String]], 
    seqop: (Map[String, Int], Seq[String]) => Map[String, Int]) = {
    val combop : (Map[String, Int], Map[String, Int]) => Map[String, Int] =
      (a, b) => b.foldLeft (a) { case (a, v) =>
        a + (v._1 -> (a.getOrElse (v._1, 0) + v._2))
      }

    data.rdd.aggregate (Map.empty[String, Int])(seqop, combop)
  }

  /** Count the nb of times each term is found
    * @param data the dataset where to estimate the frequencies
    * @return a map `token => frequency` */
  def extractVocabularyFrequencyForTokenized (data: Dataset[Seq[String]]) = 
    extractVocabularyHelper (
      data, 
      (a, xs) => xs.foldLeft (a) { (a, x) => a + (x -> (a.getOrElse (x, 0) + 1)) } )

  /** Count the number of documents countaining each terms (used in the ``document frequency`` of tfidf)
    * @param data the dataset where to estimate the frequencies
    * @return a map `token => frequency` */
  def extractVocabularyDocFrequencyForTokenized (data: Dataset[Seq[String]]) =
    extractVocabularyHelper (
      data, 
      (a, xs) => xs.toSet.foldLeft (a) { (a, x) => a + (x -> 1) } )

  /** Save vocabulary statistics computed on [[manuscripts]]. 
    * Both the abstracts and titles are used. 
    * @param maybeFrequencySavePath where to save raw term frequencies. 
    * raw term frequencies are the number of time a term (token) is seen, either in an abstract or a title.
    * @param maybeDocFrequencySavePath where to save term doc frequencies
    *   (nb of documents into which a term may be found, either in the abstract or the title)
    * @param maybeIdfWeightSavePath where to save idf weight for each term. 
    * idf is defined as `log (docfrequency/nbdocs)`
    * @return `(vocabularyFrequency, vocabularyDocFrequency)` maps */
  def extractVocabulary (
    maybeFrequencySavePath: Option[String] = None, 
    maybeDocFrequencySavePath: Option[String] = None, 
    maybeIdfWeightSavePath: Option[String]
  ) = {
    val vocabularyFrequencyAbstracts = 
      extractVocabularyFrequencyForTokenized (pr (pc getPathAbstractsTokenized).
        map (_ getSeq[String](1)))
    val vocabularyFrequencyTitles = 
      extractVocabularyFrequencyForTokenized (pr (pc getPathTitlesTokenized).
        map (_ getSeq[String](1)))
    val vocabularyFrequency = 
      vocabularyFrequencyAbstracts.
        foldLeft (vocabularyFrequencyTitles) { case (a, (term, freq)) =>
          a + (term -> (a.getOrElse (term, 0) + freq)) }
    maybeFrequencySavePath foreach {
      sc.parallelize (vocabularyFrequency.toSeq).toDF ("token", "freq").
        write.mode ("overwrite").option ("path", _) save }
    
    val vocabularyDocFrequencyAbstracts = 
      extractVocabularyDocFrequencyForTokenized (pr (pc getPathAbstractsTokenized).
        map (_ getSeq[String](1)))
    val vocabularyDocFrequencyTitles = 
      extractVocabularyDocFrequencyForTokenized (pr (pc getPathTitlesTokenized).
        map (_ getSeq[String](1)))
    val vocabularyDocFrequency = 
      vocabularyDocFrequencyAbstracts.
        foldLeft (vocabularyDocFrequencyTitles) { case (a, (term, _)) => a + (term -> 1) }
    maybeDocFrequencySavePath foreach {
      sc.parallelize (vocabularyDocFrequency.toSeq).toDF ("token", "freq").
        write.mode ("overwrite").option ("path", _) save }

    maybeIdfWeightSavePath foreach {
      val nDocs: Double = pr (pc getPathAbstractsTokenized) count
      val idfWeigths = vocabularyDocFrequency.
        map { case (w, n) => (w, Math.log (nDocs / n)) }.
        toSeq
      sc.parallelize (idfWeigths).toDF ("token", "weight").
        write.mode ("overwrite").option ("path", _) save
    }
    (vocabularyFrequency, vocabularyDocFrequency)
  }

  /** Computes a Word2Vec embedding from a dataset of sequences of tokens.
    * @param data 
    * @param embeddingSize dimension of the embedding
    * @param maybeSavePath where to save the embedding (a (String, Array[Float]) dataframe 
    * @return the Word2Vec model */
  def extractWordEmbedding_1 (
    data: Dataset[Seq[String]], 
    embeddingSize: Int, 
    maybeSavePath: Option[String]
  ) : Word2VecModel = {
    var model: Word2VecModel = null
    try {
      data cache
      val w2v = new Word2Vec ()
      w2v.setVectorSize (embeddingSize)
      model = w2v fit data.rdd
    }
    finally {
      data unpersist
    }
    
    maybeSavePath foreach { path =>
      session.sparkContext.
        parallelize (model.getVectors.toList).toDF.
        write.mode("overwrite").option ("path", path) save
    }
    model
  }

  /** Compute a Word2Vec embedding from a tokenized dataset. 
    * @param data 
    * @param embeddingSize dimension of the embedding
    * @param maybeSavePath where to save the embedding (a (String, Array[Float]) dataframe 
    * @return the Word2Vec model */
  def extractWordEmbedding (
    data: Dataset[TokenizedRecord], 
    embeddingSize: Int, 
    maybeSavePath: Option[String] = None
  ) : Word2VecModel =
    extractWordEmbedding_1 (data.map {_ terms}, embeddingSize, maybeSavePath)

  /** Creates a weighted Bag of Words from a tokenized dataset. 
    * Keys in the input dataset may contain duplicates. 
    * @param data
    * @param termWeights a map from terms to weight
    * @param maybeUnknownTermWeight the weight of a term that is not found in `termWeights` 
    * @param maybeSavePath wher to write the weighted dataset 
    * @return a dataset of [[WeightedTermRecord]] */
  def createWeighted (
    data: Dataset[TokenizedRecord], 
    termWeights: Map[String, Double],
    maybeUnknownTermWeight: Option[Double],
    maybeSavePath: Option[String]
  ) : Dataset[WeightedTermRecord] = {
    val unknownTermWeight = maybeUnknownTermWeight.getOrElse (
      termWeights.foldLeft (0.0) { case (a, (_, weight)) => Math.max (a, weight) } )
    val bTermWeights = sc.broadcast (termWeights)
    
    val ret = data.map { r => (r.id, r.terms) }.
      rdd.
      groupByKey.
      map { case (id, l) => (
        id,
        l.toSeq.flatMap { x => x }.
          foldLeft (Map.empty[String, Int]) { (a, t) => 
            a + (t -> (a.getOrElse (t, 0) + 1)) } )
      }.mapPartitions { it =>
        val termWeights: Map[String, Double] = bTermWeights.value        
        it map { case (id, localDic) =>
          val totalTermsInDoc: Double = 
            localDic.foldLeft (0) { case (a, (_, freq)) => a + freq }
          WeightedTermRecord (
            id,
            localDic.foldLeft (Map.empty[String, Double]) { case (a, (term, freq)) =>
              val tf = freq / totalTermsInDoc
              val idf = termWeights.getOrElse (term, unknownTermWeight)
              a + (term -> tf*idf)
            } ) } }.
      toDF.as[WeightedTermRecord]

    maybeSavePath foreach { ret.write.mode ("overwrite").option ("path", _) save }
    ret
  }

  /** Creates a weighted Bag of Words from a tokenized dataset. 
    * Keys in the input dataset may contain duplicates. 
    * @param data
    * @param termWeightsDataset a dataset of (terms, weight) pairs
    * @param maybeUnknownTermWeight the weight of a term that is not found in `termWeights` 
    * @param maybeSavePath wher to write the weighted dataset 
    * @return a dataset of [[WeightedTermRecord]] */
  def createWeighted (
    data: Dataset[TokenizedRecord], 
    termsWeightsDataset: Dataset[(String, Double)], 
    maybeUnknownTermWeight: Option[Double],
    maybeSavePath: Option[String]
  ) : Dataset[WeightedTermRecord] =
    createWeighted (data, termsWeightsDataset.collect.toMap, maybeUnknownTermWeight, maybeSavePath)

  /** Creates a weighted Bag of Words from a tokenized dataset. 
    * Keys in the input dataset may contain duplicates. 
    * @param data
    * @param termWeightsPath a path to a dataset of (terms, weight) pairs (default: [[path_tokensIdfWeights]])
    * @param maybeUnknownTermWeight the weight of a term that is not found in `termWeights` 
    * @param maybeSavePath wher to write the weighted dataset 
    * @return a dataset of [[WeightedTermRecord]] */
  def createWeighted (
    data: Dataset[TokenizedRecord], 
    termsWeightsPath: String = pc getPathTokensIdfWeights, 
    maybeUnknownTermWeight: Option[Double] = None,
    maybeSavePath: Option[String] = None
  ) : Dataset[WeightedTermRecord] = 
    createWeighted (data, pr (termsWeightsPath).as[(String, Double)], maybeUnknownTermWeight, maybeSavePath)

  /** Compute the embedding of manuscripts as the weighted average of the embeddings of its terms.
    * @param weightedBoWData the weighted  terms of the manuscripts
    * @param wordEmbeddingData the word embeddings
    * @param maybeSavePath where to write the new dataset
    * @return the new dataset */
  def computeManuscriptsEmbeddings (
    weightedBoWData: Dataset[WeightedTermRecord], 
    wordEmbeddingData: Dataset[(Term, Seq[Float])], 
    maybeSavePath: Option[String] = None
  ) : Dataset[EmbeddingRecord[Double]]= {
    val weightedBoWFlattenData = 
      weightedBoWData.flatMap { r =>
        r.weights.toSeq.map { case (term, weight) => (term, (r.id, weight)) }
      }.toDF ("term", "payload")
    
    val weightsEmbeddingJoinData = 
      wordEmbeddingData.toDF ("term", "embedding").
        join (weightedBoWFlattenData, "term").
        select ("embedding", "payload").
        as[(Seq[Float], (ManuscriptId, Double))].
        map { case (termEmbedding, (manuscriptId, termWeight)) =>
          (manuscriptId, (termWeight, for (e <- termEmbedding) yield e*termWeight)) }

    // Aggregate each term embeddings
    val tfidf_embedding_agg_ds = weightsEmbeddingJoinData.rdd.reduceByKey { (a, b) =>
      (a._1 + b._1, (a._2 zip b._2) map { x => x._1 + x._2 })
    }
    // Normalize
    val manuscriptsEmbedding = 
      tfidf_embedding_agg_ds.mapValues { case (n, xs) => xs.map { _ / n } }.
        toDF ("id", "embedding").as[EmbeddingRecord[Double]]
    
    maybeSavePath foreach { 
      manuscriptsEmbedding.write.mode("overwrite").option ("path", _) save }
    
    manuscriptsEmbedding
  }

  /** Compute the embedding of manuscripts as the weighted average of the 
    * embeddings of its terms. The manuscript dataset as a dataset of pairs. 
    * the first member of the pair is the id, the second member is the 
    * text content. The data is converted to weighted BoW before being 
    * embedded.
    * @param data the manuscript dataset
    * @param wordEmbeddingData the word embeddings
    * @param termWeights a map term => weight (typically the idf of the term)
    * @param maybeUnknownTermWeight an optional term weight value to use when a
    * term in the manuscript dataset does not have a weight in termWeights
    * @param maybeTransformer an optional token transformer to use at tokenization 
    * time 
    * @param maybeSavePath where to write the new dataset
    * @return the new dataset */
  def computeManuscriptsEmbeddings (
    data: Dataset[(String, String)],
    wordEmbeddingData: Dataset[(String, Seq[Float])],
    termWeights: Map[String, Double],
    maybeUnknownTermWeight: Option[Double],
    maybeTransformer: Option[Transformer],
    maybeSavePath: Option[String]
  ) : Dataset[EmbeddingRecord[Double]] = {
    val tokenized : Dataset[TokenizedRecord] =
      tokenizePerSentence (data, maybeTransformer, maybeSavePath = None)
    val weightedBoW : Dataset[WeightedTermRecord] =
      createWeighted (
        tokenized, termWeights,
        maybeUnknownTermWeight, maybeSavePath = None)
    computeManuscriptsEmbeddings (
      weightedBoW,
      wordEmbeddingData.as[(String, Seq[Float])], maybeSavePath)
  }

  /** Compute the embedding of manuscripts as the weighted average of the 
    * embeddings of its terms. The manuscript dataset as a dataset of pairs. 
    * the first member of the pair is the id, the second member is the 
    * text content. The data is converted to weighted BoW before being 
    * embedded.
    * @param data the manuscript dataset
    * @param wordEmbeddingData the word embeddings
    * @param weightData a dataset of (term => weight) pairs 
    * (typically the idf of the term)
    * @param maybeUnknownTermWeight an optional term weight value to use when a
    * term in the manuscript dataset does not have a weight in termWeights
    * @param maybeTransformer an optional token transformer to use at tokenization 
    * time 
    * @param maybeSavePath where to write the new dataset
    * @return the new dataset */
  def computeManuscriptsEmbeddings (
    data: Dataset[(String, String)],
    wordEmbeddingData: Dataset[(String, Seq[Float])],
    weightData: Dataset[(String, Double)],
    maybeUnknownTermWeight: Option[Double],
    maybeTransformer: Option[Transformer],
    maybeSavePath: Option[String]
  ) : Dataset[EmbeddingRecord[Double]] = {
    val termWeights = weightData.collect.toMap[String, Double]
    computeManuscriptsEmbeddings (data, wordEmbeddingData, termWeights, 
      maybeUnknownTermWeight, maybeTransformer, maybeSavePath)
  }

  /** Compute the embedding of manuscripts as the weighted average of the 
    * embeddings of its terms. The manuscript dataset as a dataset of pairs. 
    * the first member of the pair is the id, the second member is the 
    * text content. The data is converted to weighted BoW before being 
    * embedded.
    * @param data the manuscript dataset
    * @param wordEmbeddingData the word embeddings
    * @param weightPath a path to a dataset of (term => weight) pairs 
    * (typically the idf of the term)
    * @param maybeUnknownTermWeight an optional term weight value to use when a
    * term in the manuscript dataset does not have a weight in termWeights
    * @param maybeTransformer an optional token transformer to use at tokenization 
    * time 
    * @param maybeSavePath where to write the new dataset
    * @return the new dataset */
  def computeManuscriptsEmbeddings (
    data: Dataset[(String, String)],
    wordEmbeddingData: Dataset[(String, Seq[Float])],
    weightPath: String,
    maybeUnknownTermWeight: Option[Double],
    maybeTransformer: Option[Transformer],
    maybeSavePath: Option[String]
  ) : Dataset[EmbeddingRecord[Double]] = {
    val termData = pr (weightPath).as[(String, Double)]
    computeManuscriptsEmbeddings (data, wordEmbeddingData, termData,
      maybeUnknownTermWeight, maybeTransformer, maybeSavePath)
  }

  /** Transforms a sequences of tokens to a weighted bag of words
    * @param tokens the input tokens
    * @param weightMap a map term => weight (typically the idf of the term)
    * @return a map term => weight */
  def computeTokensSeqWeightedBoW (
    tokens: Seq[String], 
    weightMap: Map[String, Double]
  ) : Map[String, Double] = {
    val tokenCount: Double = tokens size
    val tokenFrequencies = 
      tokens.foldLeft (tokens.map { (_, 0) }.toMap) { (a, t) => 
        a + (t -> (a.getOrElse (t, 0) + 1)) }.toSeq
    tokenFrequencies.map { case (w, f) =>
      val tokenWeight: Double = weightMap.getOrElse (w, 0.0)
      (w, tokenWeight * (f / tokenCount))
    }.toMap
  }

  /** Transforms a sequences of tokens to a weighted bag of words
    * @param tokens the input tokens
    * @param weightData a dataset of (term => weight) pairs 
    * (typically the idf of the term)
    * @return a map term => weight */
  def computeTokensSeqWeightedBoW (
    texts: Seq[String], 
    weightData: Dataset[(String, Double)]
  ) : Map[String, Double] =
    computeTokensSeqWeightedBoW (texts, weightData.collect.toMap)

  /** Computes the weighted bags of words of a sequence of texts. 
    * Each text in ths sequence gets its own BoW.
    * @param texts the sequences of texts to transform
    * @param weightData a dataset of (term => weight) pairs 
    * @return a sequence of weighted BoW */
  def computeTextWeightedBoW (
    texts: Seq[String], 
    weightData: Dataset[(String, Double)]
  ) : Seq[Map[String, Double]] =
    computeTextWeightedBoW (texts, weightData.collect.toMap)

  /** Computes the weighted bags of words of a sequence of texts. 
    * Each text in ths sequence gets its own BoW.
    * @param texts the sequences of texts to transform
    * @param weightMap a map of (term => weight) pairs 
    * @return a sequence of weighted BoW */
  def computeTextWeightedBoW (
    texts: Seq[String], 
    weightMap: Map[String, Double]
  ) : Seq[Map[String, Double]] =
    tokenizeTextPerSentence (texts) map { ts => computeTokensSeqWeightedBoW (ts, weightMap) }

  /** Compute the embedding of a sequence of terms as a 
    * weighted average of the embeddings of the terms 
    * @param tokens to token sequence to embed
    * @param wordEmbeddingData a words embedding dataset
    * @param weightMap a map of (term => weight) pairs
    * @return the embedding vector as an array */
  def computeTokensSeqEmbedding (
    tokens: Seq[String], 
    wordEmbeddingData: Dataset[WordEmbeddingRecord], 
    weightMap: Map[String, Double]
  ) : Array[Double] = {
    val tokenWeights = computeTokensSeqWeightedBoW (tokens, weightMap)
    val unnormalizedEmbedding =
      sc.parallelize (tokenWeights.toSeq).
        toDF ("word", "weight").
        join (wordEmbeddingData, "word").
        select ("weight", "embedding").as[(Double, Array[Float])].
        collect.
        map { case (w, emb) => (w, emb.map (_ * w)) }.
        foldLeft ((0.0, Array.empty[Double])) { (a, x) =>
          (   a._1 + x._1,
            if (a._2.isEmpty) x._2
            else a._2.zip (x._2).map {y => y._1 + y._2 }
          ) }
    unnormalizedEmbedding._2.map { _ / unnormalizedEmbedding._1 }
  }

  /** computes the embedding of a sequence of texts 
    * as the weighted average of their terms. 
    * Tokenization is the simpler one 
    * (no transformation nor pattern replacement 
    * @param texts
    * @param wordEmbeddingData
    * @param weightData
    * @return a sequence of embedding */
  def computeTextEmbedding (
    texts: Seq[String], 
    wordEmbeddingData: Dataset[WordEmbeddingRecord], 
    weightData: Dataset[(String, Double)]
  ) : Seq[Array[Double]] =
    computeTextEmbedding (texts, wordEmbeddingData, weightData.collect.toMap)
  
  /** computes the embedding of a sequence of texts 
    as the weighted average of their terms. 
    * Tokenization is the simpler one 
    (no transformation nor pattern replacement 
    * @param texts
    * @param wordEmbeddingData
    * @param weightMap
    * @return a sequence of embedding */
  def computeTextEmbedding (
    texts: Seq[String], 
    wordEmbeddingData: Dataset[WordEmbeddingRecord], 
    weightMap: Map[String, Double]
  ) : Seq[Array[Double]] =
    tokenizeTextPerSentence (texts) map { ts => computeTokensSeqEmbedding (ts, wordEmbeddingData, weightMap) }

  /** creates a One Hot Encoding data to train classifier on areas.
    * @param manuscriptsEmbeddingPath 
    * @param maybeSavePath where to write OHE data
    * @param maybeAreaIndexSavePath where to write the data informing on 
    * the position of each area in the OHE vector
    * @return a pair of these two data sets */
  def createOneHotAreaEncodingForClassification (
    manuscriptsEmbeddingPath: String, 
    maybeSavePath: Option[String] = None, 
    maybeAreaIndexSavePath: Option[String] = None
  ) : (Dataset[OHETrainingDataRecord], Dataset[(String, Int)]) = {
    val manuscriptsEmbedding = pr (manuscriptsEmbeddingPath).toDF ("eid", "embedding")
    val areaToIndexMap : Map[String, Int] = flattenSubjAreas.
      select ("area").
      map { _ getString 0 }.
      distinct.
      collect.
      sorted.
      zipWithIndex.
      toMap

    val areaToIndexDS = sc.parallelize (areaToIndexMap.toSeq).toDF ("area", "areaIndex")
    val flattenSubjAreasIndex = 
      flattenSubjAreas.join (areaToIndexDS, "area").select ("eid", "areaIndex")
    val OneHotEnc = 
      flattenSubjAreasIndex.as[(String, Int)].rdd.
        aggregateByKey (Array.fill[Int](numberOfAreas)(0)) (
          (a, i) => { a (i) = 1; a },
          (a, b) => (a zip b) map { case (x, y) => x + y } ).
        toDF("eid", "hotarea").
        join (manuscriptsEmbedding, "eid").
        toDF ("id", "ohe", "features").
        as[OHETrainingDataRecord]
    
    maybeSavePath foreach { OneHotEnc.write.mode ("overwrite").option ("path", _) save }
    maybeAreaIndexSavePath foreach { areaToIndexDS.write.mode ("overwrite").option ("path", _) save }
    (OneHotEnc, areaToIndexDS.as[(String, Int)])
  }

  /** Computes a logistic regression classifier to predict one area, 
    * identified by its index.
    * @param areaIndex
    * @param OHEData
    * @param trainProportion split train/test 
    * @return a pair of the correct predictions and testing data set */ 
  def areaLRModel (
    areaIndex: Int, 
    OHEData: Dataset[OHETrainingDataRecord],
    trainProportion: Double
  ) : (Long, Long) = {
    val labeledPointsData = 
      OHEData.map { x =>
          LabeledPoint (x.ohe(areaIndex), Vectors dense (x.features toArray)) }

    val Array(training, testing) = 
      labeledPointsData.randomSplit (Array (trainProportion, 1.0 - trainProportion))
    val lr = new LogisticRegression ()
    val lrModel = lr.fit (training)
    (lrModel.transform (testing).filter {x => x.getDouble (0) == x.getDouble(4)}.count,
      testing count)
  }

  /** Computes a logistic regression classifier to predict one area, 
    * identified by its index.
    * @param areaIndex
    * @param OHEPath the path to the OHE dataset
    * @param trainProportion split train/test (default: 0.8 => train on 80% of the DS)
    * @return a pair of the correct predictions and testing data set */ 
  def areaLRModel (
    areaIndex: Int, 
    OHEPath: String = pc getPathAreaOHE4Classification, 
    trainProportion: Double=0.8
  ): (Long, Long) =
    areaLRModel (areaIndex, pr (OHEPath).as[OHETrainingDataRecord], trainProportion)

  /** Computes a logistic regression model for all area.
    * @param OHEData the data to train on and test on
    * @param maybeAreaOHEIndexPath: the path to the area => index file, used
    * to fill the result with the name (otherwise the index is used)
    * @param trainProportion
    * @param maybePerformancesSavePath
    * @return the performances as a dataset */
  def allAreaOHELRModel (
    OHEData: Dataset[OHETrainingDataRecord],
    maybeAreaOHEIndexPath: Option[String], 
    trainProportion: Double, 
    maybePerformancesSavePath: Option[String]
  ) : Dataset[OHEClassificationModelPerformancesRecord] = {
    val modelPerformances = 
      (0 until numberOfAreas).
        foldLeft (List.empty[(Int, Long, Long, Double)]) { (a, i) =>
          val (fitCount, testCount) = areaLRModel (i, OHEData, trainProportion)
          a :+ (i, fitCount, testCount, (100.0*fitCount) / testCount) }

    val modelPerformancesDF = sc.parallelize (modelPerformances).
      toDF ("areaIndex", "fitcount", "totalcount", "accuracy")
    val modelPerformancesWithLabelsDF =
      maybeAreaOHEIndexPath.
        map { path => pr (path).join (modelPerformancesDF, "areaIndex") }.
        getOrElse (modelPerformancesDF.withColumn ("label", $"areaIndex" cast StringType)).
        select ("label", "fitcount", "totalcount", "accuracy")
    maybePerformancesSavePath foreach { modelPerformancesWithLabelsDF.write.mode ("overwrite").option ("path", _) save }
    modelPerformancesWithLabelsDF.as[OHEClassificationModelPerformancesRecord]
  }

  /** Computes a logistic regression model for all area.
    * @param OHEPath the data to train on and test on
    * @param maybeAreaOHEIndexPath: the path to the area => index file, used
    * to fill the result with the name (otherwise the index is used)
    * @param trainProportion
    * @param maybePerformancesSavePath
    * @return the performances as a dataset */
  def allAreaOHELRModel (
    OHEPath: String = pc getPathAreaOHE4Classification, 
    maybeAreaOHEIndexPath: Option[String] = None, 
    trainProportion: Double=0.8, 
    maybePerformancesSavePath: Option[String] = None
  ) : Dataset[OHEClassificationModelPerformancesRecord] = 
    allAreaOHELRModel (pr (OHEPath).as[OHETrainingDataRecord],
      maybeAreaOHEIndexPath, trainProportion, maybePerformancesSavePath)

  /** For a given set of subject areas, creates an OHE dataset 
    * of issns for journals of these areas 
    * @param areas the set of subject areas used as filter criteria
    * @param maybeSavePath where to save the OHE dataset
    * @return a dataframe of pairs (String, Array[Int]) 
    * where the first component is an eid */
  def createISSN_OHEDataset (
    areas: Seq[String], 
    maybeSavePath: Option[String] = None) : DataFrame = {
    // ids of manuscripts for these areas
    val eids = 
      flattenSubjAreas.
        where ($"area".isin (areas: _*)).
        select ("eid").
        distinct
    // table (issn, index in the OHE vector)
    val issns = manuscripts.
      select ("eid", "issn").
      join (eids, "eid").
      groupBy ("issn").
      count.
      orderBy ($"count" desc).
      select ("issn").
      map {_ getString 0}.
      collect.
      zipWithIndex
    val issnDF = sc.parallelize (issns).toDF ("issn", "index")
    val N = issns.size
    val ret = manuscripts.
      select ("eid", "issn").
      join (issnDF, "issn").
      map { x =>
        val eid = x getString 1
        val index = x getInt 2
        val oheArray = Array.fill[Int](N)(0)
        oheArray (index) = 1
        (eid, oheArray)
      }.
      toDF ("id", "ohe")
    maybeSavePath foreach { ret.write.mode ("overwrite").option ("path", _) save }
    ret
  }

  /** train a logistic regression model on an OHE dataset. 
    * The classification is binary, one against others
    * @param areaIndex which class to classify
    * @param OHEdata training and testing data
    * @param trainProportion proportion of training data
    * @return a triple where the first component is the model, 
    * the second component is the training data 
    * and the third is the testing data */ 
  def trainOHELRModel (
    areaIndex: Int,
    OHEData: Dataset[OHETrainingDataRecord],
    trainProportion: Double
  ) : (LogisticRegressionModel, Dataset[LabeledPoint], Dataset[LabeledPoint]) = {
    val labeledPointsData =
      OHEData.map { x =>
        LabeledPoint (x.ohe(areaIndex), Vectors dense (x.features toArray)) }

    val Array(training, testing) =
      labeledPointsData.randomSplit (Array (trainProportion, 1.0 - trainProportion))
    val lr = new LogisticRegression ()
    val lrModel = lr.fit (training)
    (lrModel, training, testing)
  }
    
  /** Evaluate a logistic regression model on a set of test datasets
    * @param model the model
    * @param evalDatasets the datasets on which to test
    * @return a sequence of estimates of model performance
    */
  def evaluateLRModel (
    model: LogisticRegressionModel, 
    evalDatasets: Dataset[LabeledPoint]*
  ) : Seq[ModelEvaluationRecord] = {
    for (d <- evalDatasets) yield {
      val e = model.transform (d).as [LRModelTransformRecord]
      e.rdd.aggregate (ModelEvaluationRecord empty) (
        (mer: ModelEvaluationRecord, r: LRModelTransformRecord) => mer + ModelEvaluationRecord (
                    numberOfCase = 1,
                    numberOfPositive = if (r.label > 0.5) 1 else 0,
                    numberOfPredictedPositive = if (r.prediction > 0.5) 1 else 0,
                    numberOfTruePositive = if (r.label > 0.5 && r.prediction > 0.5) 1 else 0,
                    numberOfTrueNegative = if (r.label < 0.5 && r.prediction < 0.5) 1 else 0 ) ,
        (mer1: ModelEvaluationRecord, mer2: ModelEvaluationRecord) => mer1  + mer2
      )
    }
  }
    
  /** compute the L2 distances between a bunch of vectors and each elements in an embedding dataset.
    * @param xs the vectors from which to compute the norm
    * @param embedding
    * @return a dataset of (elementid, [norms2]) pairs */ 
  def computeL2Square (
    xs: Seq[Seq[Double]], 
    embedding: Dataset[EmbeddingRecord[Double]]
  ) : Dataset[(ManuscriptId, Seq[Double])] = {
    val kernel = (x: Seq[Double], y: Seq[Double]) =>
    (x zip y).
      map { case (x, y) => x - y }.
      foldLeft (0.0) { (a, x) => a + x*x }
    embedding map { r => (r.id, xs map { kernel (_ , r embedding) }) }
  }
    
  /** Find the n most similar embeddings to input vectors.
    * @param xs the set of vectors for which we have to find the n most similar
    * @param embedding the embedding to search in
    * @param n the number of elements to retrieve for each input vector
    * @param o an ordering for ranking similarities
    * @return a sequence of sequences s_i, one sequence for each input vector.
    * each element of a subsequence is a pair (id, distance) */
  def nSimilars (
    xs: Seq[Seq[Double]],
    embedding: Dataset[EmbeddingRecord[Double]], 
    n: Int,
    o: scala.math.Ordering[(String, Double)]
  ) : Seq[Seq[(String, Double)]] = {
    import scala.collection.immutable.TreeSet
    val sqDistDataset : Dataset[(String, Seq[Double])] = computeL2Square (xs, embedding)
    sqDistDataset.rdd.
      aggregate (Seq.fill[TreeSet[(String, Double)]](xs size)(TreeSet.empty (o))) (
        (tsets: Seq[TreeSet[(String, Double)]], sims: (ManuscriptId, Seq[Double])) => {
          (tsets zip sims._2) map { case (ts, s) => (ts + ((sims._1, s))) slice (0, n) }
        },
        (tsets1: Seq[TreeSet[(String, Double)]], tsets2: Seq[TreeSet[(String, Double)]]) => {
          (tsets1 zip tsets2) map { case (ts1, ts2) => (ts1 ++ ts2) slice (0, n) }
        }
      ) map { _ toSeq }
  }

  /** Find the n most similar embeddings to input vectors.
    * @param xs the set of vectors for which we have to find the n most similar
    * @param embedding the embedding to search in
    * @param n the number of elements to retrieve for each input vector
    * @return a sequence of sequences s_i, one sequence for each input vector.
    * each element of a subsequence is a pair (id, distance) */
  def mostSimilars (
    xs: Seq[Seq[Double]], 
    embedding: Dataset[EmbeddingRecord[Double]], 
    n: Int
  ) : Seq[Seq[(String, Double)]] = {
    import scala.collection.immutable.TreeSet
    implicit val o = new scala.math.Ordering[(String, Double)] {
      def compare (a: (String, Double), b: (String, Double)) = 
        if (a._2 < b._2) -1 else if (a._2 > b._2) 1 else 0
    }

    val sqDistDataset : Dataset[(String, Seq[Double])] = computeL2Square (xs, embedding)
    sqDistDataset.rdd.aggregate (Seq.fill[TreeSet[(String, Double)]](xs size)(TreeSet.empty)) (
      (tsets: Seq[TreeSet[(String, Double)]], sims: (ManuscriptId, Seq[Double])) => {
        (tsets zip sims._2) map { case (ts, s) => (ts + ((sims._1, s))) slice (0, n) }
      },
      (tsets1: Seq[TreeSet[(String, Double)]], tsets2: Seq[TreeSet[(String, Double)]]) => {
        (tsets1 zip tsets2) map { case (ts1, ts2) => (ts1 ++ ts2) slice (0, n) }
      }
    ) map { _ toSeq }
  }

  /** Given a keyed dataset of coordinates, computes the centroids of each key
    * @param data the keyed data sets
    * @param dimension the size of the vectors
    * @return a dataFrame a the centroids */
  def computeCentroids (data: Dataset[(String, Seq[Double])], dimension: Int) : DataFrame = {
    data.rdd.aggregateByKey ((0, Seq.fill[Double](dimension)(0.0))) (
      (a, x) => (a._1 + 1, a._2.zip (x) map {
        case (e1, e2) => e1 + e2
      }),
      (a1, a2) => (a1._1 + a2._1, a1._2.zip (a2._2) map {
        case (e1, e2) => e1 + e2
      })
    ).mapValues { case (n, v) => v map ( _ / n) }.toDF ("id", "embedding")
  }
    
  /** Compute the centroids embedding of the journals
    * @param manuscripts a dataframe of manuscripts
    * @param embeddings the manuscripts embeddings dataframe
    * @param dimension  the dimension of the embedding
    * @return a dataframe of the journals embeddings */
  def computeJournalCentroidsFromManuscriptsEmbeddings (
    manuscripts: DataFrame, 
    embeddings: DataFrame, 
    dimension: Int,
    maybeSavePath: Option[String]
  ) : DataFrame = {
    val centroids = computeCentroids (
      embeddings.
        join (manuscripts select ("eid", "issn"), $"id" === $"eid").
        select ("issn", "embedding").
        as[(String, Seq[Double])],
      dimension
    ).toDF ("id", "embedding")

    maybeSavePath foreach { 
      centroids.write.mode ("overwrite").option ("path", _ ) save
    }
    centroids
  }

  /** Compute the centroids embedding of the journals, split per year
    * @param manuscripts a dataframe of manuscripts
    * @param embeddings the manuscripts embeddings dataframe
    * @param dimension  the dimension of the embedding
    * @return a dataframe of the journals embeddings */
  def computeJournalCentroidsFromManuscriptsEmbeddingsPerYear (
    manuscripts: DataFrame, 
    embeddings: DataFrame, 
    dimension: Int,
    maybeSavePath: Option[String]
  ) : DataFrame = {
    val centroids = computeCentroids (
      embeddings.
        join (manuscripts select ("eid", "issn", "year"), $"id" === $"eid").
        select ("issn", "year", "embedding").
        as[(String, Short, Seq[Double])].
        map { x => (s"${x._1}:${x._2}", x._3) },
      dimension).
      map { x =>
        val Array (issn, year) = x.getString (0).split (":")
        (issn, x getSeq[Double] 1, year toShort)
      }.
      toDF ("id", "embedding", "year")
    maybeSavePath foreach { 
      centroids.write.mode ("overwrite").option ("path", _ ) save
    }
    centroids
  }

  /** Similar to computeJournalCentroidsFromManuscriptsEmbeddings, 
    * but add the subject areas in the last field of the returned dataset
    * @param manuscripts a dataframe of manuscripts
    * @param embeddings the manuscripts embeddings dataframe
    * @param dimension  the dimension of the embedding
    * @param subjAreaMap a map issn => subjectAreas
    * @param maybeSavePath where to save
    * @return a dataframe of the journals embeddings */
  def computeJournalCentroidsFromManuscriptsEmbeddingsWithSubjareas (
    manuscripts: DataFrame, 
    embeddings: DataFrame, 
    dimension: Int,
    subjAreaMap: Map[String, Seq[String]],
    maybeSavePath: Option[String]
  ) : DataFrame = {
    val centroids = 
      computeJournalCentroidsFromManuscriptsEmbeddings (
        manuscripts, embeddings, dimension, maybeSavePath)

    val bSubjAreaMap = sc.broadcast (subjAreaMap)
    val ret = centroids.mapPartitions { it =>
      val subjAreaMap = bSubjAreaMap.value
      it map { x =>
        val issn = x getString 0
        val subjareas = subjAreaMap getOrElse (issn, Seq.empty[String])
        (issn, x getSeq[Double] 1, subjareas)
      }
    }.
      toDF ("id", "embedding", "subjareas")
    maybeSavePath foreach {
      ret.write.mode ("overwrite").option ("path", _) save
    }
    ret
  }

  /** Similar to computeJournalCentroidsFromManuscriptsEmbeddingsPerYear, 
    * but add the subject areas in the last field of the returned dataset
    * @param manuscripts a dataframe of manuscripts
    * @param embeddings the manuscripts embeddings dataframe
    * @param dimension  the dimension of the embedding
    * @param subjAreaMap a map issn => subjectAreas
    * @param maybeSavePath where to save
    * @return a dataframe of the journals embeddings */
  def computeJournalCentroidsFromManuscriptsEmbeddingsPerYearWithSubjareas (
    manuscripts: DataFrame, 
    embeddings: DataFrame, 
    dimension: Int,
    subjAreaMap: Map[String, Seq[String]],
    maybeSavePath: Option[String]
  ) : DataFrame = {
    val centroids = 
      computeJournalCentroidsFromManuscriptsEmbeddingsPerYear (
        manuscripts, embeddings, dimension, None)

    val bSubjAreaMap = sc.broadcast (subjAreaMap)
    val ret = centroids.mapPartitions { it =>
      val subjAreaMap = bSubjAreaMap.value
      it map { x =>
        val issn = x getString 0
        val subjareas = subjAreaMap getOrElse (issn, Seq.empty[String])
        (issn, x getSeq[Double] 1, x getShort 2, subjareas)
      }
    }.
      toDF ("id", "embedding", "year", "subjareas")
    maybeSavePath foreach {
      ret.write.mode ("overwrite").option ("path", _) save
    }
    ret
  }

  /** Tokenizes abstracts of the manuscripts and save the results
    * Should not be used (deprecated) 
    * Use doitSaveTokenizePerSentence instead */
  def doitSaveTokenizedAbstracts =
    saveTokenizedAbstracts (manuscripts.toDF, "s3://wads/epfl/thy/manuscripts-content-tokenized-abstracts")

  /** Tokenizes abstract and title of manuscripts and save the results
    * with respect to the configuration */
  def doitSaveTokenizePerSentence = {
    tokenizePerSentence (
      manuscripts.
        select ("eid", "abstr").
        map {x => (x getString 0, x getString 1)},
      None, Some (pc getPathAbstractsTokenized))

    tokenizePerSentence (
      manuscripts.
        select ("eid", "title").
        map {x => (x getString 0, x getString 1)},
      None, Some (pc getPathTitlesTokenized))
  }
  
  /** extracts words embedding and save it with respect to the configuration */
  def doitExtractWordsEmbedding =
    extractWordEmbedding (session.read.parquet (pc getPathAbstractsTokenized).as[TokenizedRecord], 
      config.embedding.dimension, 
      Some (pc getPathWordEmbeddingOnAbstracts))

  /** extracts terms total frequencies, document frequencies and 
    * inverse document frequencies and save them with respect to the configuration */
  def doitExtractVocabulary =
    extractVocabulary (Some (pc getPathTokensTotalFrequencies), 
      Some (pc getPathTokensDocFrequencies), Some (pc getPathTokensIdfWeights))

  /** transforms manuscripts to a dataset of weighted BoW (from abstract only)
    * and save the rsult with respect to the configuration */
  def doitSaveTFIDFWeighted =
    createWeighted (pr (pc getPathAbstractsTokenized).as[TokenizedRecord], 
      maybeSavePath = Some (pc getPathTfidfWeights))
 
  /** computes manuscripts embedding from abstracts
    * and save the result with respect to the configuration */
  def doitExtractManuscriptsEmbedding = {
    val tfidf_ds = pr (pc getPathTfidfWeights).as[WeightedTermRecord]
    val wordEmbedding_ds = pr (pc getPathWordEmbeddingOnAbstracts).as[(String, Seq[Float])]
    computeManuscriptsEmbeddings (tfidf_ds, wordEmbedding_ds, Some(pc getPathManuscriptsEmbedding))
  }

  def doitExtractJournalsEmbedding = {
    computeJournalCentroidsFromManuscriptsEmbeddings (
      manuscripts.toDF,
      manuscriptsEmbeddings.toDF,
      config.embedding.dimension,
      Some (pc getPathJournalsEmbedding))
    computeJournalCentroidsFromManuscriptsEmbeddingsPerYear (
      manuscripts.toDF,
      manuscriptsEmbeddings.toDF,
      config.embedding.dimension,
      Some (pc getPathJournalsEmbeddingPerYear))
  }

  /** creates dataset suitable to train models that classify subject areas from
    * manuscrpts ambedding and save the result with respect to the configuration */
  def doitExtractAreaOHE4Classif =
    createOneHotAreaEncodingForClassification (
      pc getPathManuscriptsEmbedding, 
      Some (pc getPathAreaOHE4Classification), 
      Some (pc getPathAreaOHEIndex))
  
  /** Compute classification models for all subject area */
  def doitAllAreaOHELRModels =
    allAreaOHELRModel (
      pc getPathAreaOHE4Classification, 
      Some (pc getPathAreaOHEIndex), 
      0.8, 
      Some (pc getPathAreaOHEModelPerformances))

  /** A pipeline that create main standards artifacts: tokenised manuscripts, 
    * term frequencies, BoW, word and manuscripts embeddings,... */
  def doit = {
    doitSaveTokenizePerSentence
    doitExtractVocabulary
    doitExtractWordsEmbedding
    doitSaveTFIDFWeighted
    doitExtractManuscriptsEmbedding
    doitExtractJournalsEmbedding
    doitExtractAreaOHE4Classif
  }
}

/** Used to compute model predicting issns, restricted to some area */ 
class ISSNClassifyOneAgainstOthers (areas: Seq[String], app: EmbeddingApp) (implicit session: SparkSession) {
  import EmbeddingApp._
  import session.implicits._

  val embedding = session.read.parquet (app.config.persist getPathManuscriptsEmbedding)
  val eids = app.flattenSubjAreas.
    where ($"area" isin (areas:_*)).
    join (app.manuscripts, "eid").
    select ($"eid" as "id", $"issn")

  /** given a list of issn, train the first issn against the others.
    * @param issns,
    * @param trainProportion
    * @return a 4-uple: (# of fits on the test, size of the test, # of fits on the complement, size of the complement) */
  def classify (issns: Seq[String], trainProportion: Double = 0.8) = {
    val learn_eids = 
      app.manuscripts.
        where ($"issn" isin (issns:_*)).
        select ($"eid" as "id", $"issn")
    val complement = 
      eids.
        where (! ($"issn" isin (issns:_*))).
        join (embedding, "id").
        map { x =>
          val features = Vectors dense (x.getSeq[Double] (2).toArray)
          LabeledPoint (0.0, features)
        }
    val learn_data = 
      learn_eids.
        join (embedding, "id").
        map { x =>
          val issn = x getString 1
          val label = if (issn == issns(0)) 1.0 else 0.0
          val features = Vectors dense (x.getSeq[Double] (2).toArray)
          LabeledPoint (label, features)
        }
    val Array(training, testing) = learn_data.randomSplit (Array (trainProportion, 1.0 - trainProportion))
    val lr = new LogisticRegression ()
    val lrModel = lr.fit (training)
    (lrModel.transform (testing).filter {x => x.getDouble (0) == x.getDouble(4)}.count, testing count,
      lrModel.transform (complement).filter {x => x.getDouble (0) == x.getDouble(4)}.count, complement count)
  }

  /** compute frequencies of issns for some subject areas
    * @param areas the subject area to filter on */
  def getIssnFreq (areas: Seq[String]) =
    app.manuscripts.
      select ("eid", "issn").
      join (app.flattenSubjAreas.where ($"area" isin (areas:_*)), "eid").
      groupBy ("issn").
      count.
      orderBy ($"count" desc)
}

/** a demo of [[ISSNClassifyOneAgainstOthers$]] */
object ISSNClassifyOneAgainstOthers {
    def demo (implicit session: SparkSession) = {
      val app = new EmbeddingApp (ElsStdEmbeddingConfig)
      val x = new ISSNClassifyOneAgainstOthers (List("MATE"), app)
      x.classify (List ("09258388","01694332", "19448244", "20507488"))
    }
}

