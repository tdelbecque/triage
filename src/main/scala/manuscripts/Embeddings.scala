package com.sodad.els.triage.manuscripts

import scala.collection.JavaConversions._
import java.util.Properties
import edu.stanford.nlp.pipeline._
import edu.stanford.nlp.ling.CoreAnnotations._
import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql._
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.sql.types.StringType

/** This trait is used to transform tokens during tokenization */
trait Transformer extends java.io.Serializable {
  def transform (term: String) : String
}

class LowerCaseTransformer extends Transformer {
  def transform (term: String) = term toLowerCase
}

/** Records of datasets holding sequences of tokens, associated with an id.
  * It is typically generated with a call to [[EmbeddingApp.tokenizePerSentence]] */
case class TokenizedRecord (
  id: String,
  terms: Seq[String])

/** a manuscript (identified by id), transformed as a weighed BoW. 
  * It is typically generated by a call to createWeighted */
case class WeightedTermRecord (
    id: String,
    weights: Map[String, Double])

case class WordEmbeddingRecord (word: String, embedding: Seq[Float])

case class EmbeddingRecord[T] (id: String, embedding: Seq[T])

case class OHETrainingDataRecord (id: String, ohe: Array[Int], features: Array[Double])
 
/** Classification performance estimates for an "One Hot Encoded" target. */
case class OHEClassificationModelPerformancesRecord (
  /** the label of the class */
  label: String,
  /** number of times this class rightly was predicted */
  fitCount: Long,
  /** number of cases in the test set */
  totalCount: Long,
  /** fitcount / totalCount */
  accuracy: Double
)

/** Pattern replacement utilities */
package object replacement {
  /** Replace numerical expressions.
    * - +1.3 % => sodadpct
    * - = 1.3 => sodadnumexpr
    * - 1.3 = => sodadnumexpr
    * - 1.3 => sodadnum       */
  val defaultReplacementPatterns = {
    val floatStr = """(?:\+/-|\+-|\+|-)?\s*\d+(?:\.\d+)*"""
    List (
      s"$floatStr\\s*%".r -> " sodadpct ",
      s"[<>=]+\\s*$floatStr".r -> " sodadnumexpr ",
      s"$floatStr\\s*[<>=]+".r -> " sodadnumexpr ",
      s"$floatStr".r -> " sodadnum ",
      """\(.*?\)""".r -> " ",
      """\s+""".r -> " ")
  }

  /** Apply a sequence of pattern replacements to a record instantiated as an (id, text) pair.
    * @param r: an `(id, text)` pair
    * @param patterns: the set of replacements to apply to `text`
    * @return: an `(id, repl_text)` pair where `repl_text` is the result of the replacements applied to `text` */
  def replacePatternsInRecord (
    r: (String, String),
    patterns: Seq[(scala.util.matching.Regex, String)] = defaultReplacementPatterns
    ) : (String, String) = ( 
      r._1,
      patterns.foldLeft (r._2) { case (a, (e, r)) => e replaceAllIn (a, r) } )

  /** Replace patterns to all records of a dataset.
    * @param data: the data to process
    * @param maybeSavePath: where to save to new dataset */
  def replacePatterns(data: Dataset[(String, String)],
    patterns: Seq[(scala.util.matching.Regex, String)] = defaultReplacementPatterns,
    maybeSavePath: Option[String] = None)
    (implicit session: SparkSession): Dataset[(String, String)] = {
    import session.implicits._
    val replacement = data map (replacePatternsInRecord(_, patterns))
    maybeSavePath foreach { replacement.write.option ("path", _) save }
    replacement
  }
  
}

/** tells where all objects must retrieved and saved */
trait PersistConfig {
  /** manuscripts corpora */
  def getPathManuscripts : String
  /** manuscript abstracts tokenized */
  def getPathAbstractsTokenized : String
  /** manuscript titles tokenized */
  def getPathTitlesTokenized : String
  /** absolute frequencies of tokens (terms) in the manuscript corpora */
  def getPathTokensTotalFrequencies : String
  /** for each term, the number of documents that contains it */
  def getPathTokensDocFrequencies : String
  /** weight (for example idf) of each term in the corpora */
  def getPathTokensIdfWeights : String
  /** the manuscripts as bag of words, weighted */
  def getPathTfidfWeights : String
  /** embeddings of the words of the corpora */
  def getPathWordEmbeddingOnAbstracts : String
  /** embeddings of the papers of the corpora */
  def getPathManuscriptsEmbedding : String
  /** One Hot Encoding of areas, and manuscripts embeddings, 
    * usefull for area classifcation purpose */
  def getPathAreaOHE4Classification : String
  /** map the area label to its position in the ohe vector */
  def getPathAreaOHEIndex : String
  /** Performances of a area classification models */
  def getPathAreaOHEModelPerformances : String
}

trait EmbeddingConfig {
  def persist : PersistConfig
}

object ElsStdPersistConfig extends PersistConfig {
  val dataRepositoryPrefix = ManuscriptsApp.dataRepositoryPrefix
  def getPathManuscripts = ManuscriptsApp.path_manuscripts
  def getPathAbstractsTokenized = 
    s"$dataRepositoryPrefix/thy/manuscripts-abstracts-tokenized"
  def getPathTitlesTokenized = 
    s"$dataRepositoryPrefix/thy/manuscripts-titles-tokenized"
  def getPathTokensTotalFrequencies = 
    s"$dataRepositoryPrefix/thy/manuscripts-tokens-total-frequencies"
  def getPathTokensDocFrequencies = 
    s"$dataRepositoryPrefix/thy/manuscripts-tokens-doc-frequencies"
  def getPathTokensIdfWeights = 
    s"$dataRepositoryPrefix/thy/manuscripts-tokens-idf-weights"
  def getPathTfidfWeights = 
    s"$dataRepositoryPrefix/thy/manuscripts-tfidf-weights"
  def getPathWordEmbeddingOnAbstracts = 
    s"$dataRepositoryPrefix/thy/wordEmbeddingOnAbstracts"
  def getPathManuscriptsEmbedding = 
    s"$dataRepositoryPrefix/thy/manuscriptsEmbedding"
  def getPathAreaOHE4Classification = 
    s"$dataRepositoryPrefix/thy/areaOHE4classifData"
  def getPathAreaOHEIndex = s"$dataRepositoryPrefix/thy/areaOHEIndex"
  def getPathAreaOHEModelPerformances = 
    s"$dataRepositoryPrefix/thy/areaOHEModelPerformances"
}

object ElsStdEmbeddingConfig extends EmbeddingConfig {
  def persist = ElsStdPersistConfig
}

/** Some constants and utility functions, related to manuscripts embedding, 
  * but that does not need a spark session */
object EmbeddingApp {
  val path_manuscripts = ElsStdPersistConfig getPathManuscripts
  val dataRepositoryPrefix = ElsStdPersistConfig dataRepositoryPrefix

  val path_abstractsTokenized = ElsStdPersistConfig getPathAbstractsTokenized
  val path_titlesTokenized = ElsStdPersistConfig getPathTitlesTokenized
  val path_tokensTotalFrequencies = ElsStdPersistConfig getPathTokensTotalFrequencies
  val path_tokensDocFrequencies = ElsStdPersistConfig getPathTokensDocFrequencies
  val path_tokensIdfWeights = ElsStdPersistConfig getPathTokensIdfWeights
  val path_tfidfWeights = ElsStdPersistConfig getPathTfidfWeights
  val path_wordEmbeddingOnAbstracts = ElsStdPersistConfig getPathWordEmbeddingOnAbstracts
  val path_manuscriptsEmbedding = ElsStdPersistConfig getPathManuscriptsEmbedding
  val path_areaOneHotEncodingForClassification = ElsStdPersistConfig getPathAreaOHE4Classification
  val path_areaOneHotEncodingIndex = ElsStdPersistConfig getPathAreaOHEIndex
  val path_areaOneHotEncodingModelPerformances = ElsStdPersistConfig getPathAreaOHEModelPerformances

  /** Apply tokenization to a sequence of texts.
    * @param texts  what to tokenize
    * @param maybeTransformer  the transformer to apply to each tokens, once the tokenization is done
    * @return a sequence of sequences of token, one sequence per text in the `texts` parameter */
  def tokenizeTextPerSentence (texts: Seq[String], maybeTransformer: Option[Transformer] = None) = {
    val transform: String => String =
      maybeTransformer.
        map { _.transform _ }.
        getOrElse { x => x }
    val props = new Properties ()
    props put ("annotators", "tokenize, ssplit")
    val pipeline = new StanfordCoreNLP (props)
    texts.map { t =>
      val ann = new Annotation (t)
      pipeline annotate ann
        (for (s <- ann get classOf[SentencesAnnotation]) // split sentences
        yield (
          (for (t <- s.get (classOf[TokensAnnotation]).map { x => 
            transform (x.originalText()) }) yield t) toList
        )).
        toList.
        flatMap {x => x} // reduces a 2 levels list to a 1 level one.
    }
  }

}

/** All manuscripts embedding utilities, when a spark session is needed.
  * Per default, the manuscripts dataset is the one computed from [[ManuscriptsApp.doitExtractManuscriptsContent]]
  *  and is stored to [[ManuscriptsApp.path_manuscripts]] */ 
class EmbeddingApp (config: EmbeddingConfig) (implicit session: SparkSession) {
  type ManuscriptId = String
  type Term = String

  import EmbeddingApp._
  import session.implicits._

  val pr = session.read.parquet _
  val sc = session.sparkContext

  val pc = config.persist

  val numberOfAreas = 27

  /** the whole set of ``complete`` manuscripts */
  lazy val manuscripts = session.read.parquet(path_manuscripts)
  /** the manuscripts restricted to the titles */
  lazy val titles = manuscripts.select ("title").map (_.getString (0))
  /** the manuscripts restricted to the abstracts */
  lazy val abstracts = manuscripts.select ("abstr").map (_.getString (0))

  /** A dataframe (fields = `eid`, `area`) holding the subject areas for each paper.
    * One area per record. Computation is performed on [[manuscripts]] */
  lazy val flattenSubjAreas =
    manuscripts.select ("eid", "subjareas").flatMap { x =>
      val eid = x getString 0
      val areas: Seq[String] = x getSeq[String] 1
      areas.map { (eid, _) }
    }.
      toDF ("eid", "area")
  
  /** A dataframe (fields = `area`, `freq`, `pct`) for counting frequencies of areas.  
    * Computation is done on [[manuscripts]] */
  lazy val areaFrequencies =
    flattenSubjAreas.
      groupBy ("area").count.
      select ($"area", $"count" as "freq", ($"count" * 100.0 / manuscripts.count) as "pct").
      orderBy ($"count" desc)

  /** Given a dataframe of `(String, String)` where `_1` is an id, and `_2` is the text to
    *  tokenize, creates a record for each sentence in `_2`.
    *  @param data what to tokenize
    *  @param maybeTransformer if not `None`, a tranformer that will be applied to each token
    *  @param maybeSavePath where to write the new dataset
    *  @return tokenized dataset. */
  def tokenizePerSentence (
    data: Dataset[(String, String)],
    maybeTransformer: Option[Transformer] = None,
    maybeSavePath: Option[String]
  ) : Dataset[TokenizedRecord] = {
    val transform : String => String = maybeTransformer match {
      case Some (s) => s.transform _
      case _ => x => x
    }

    val tokenized = data mapPartitions { it =>
      val props = new Properties ()
      props put ("annotators", "tokenize, ssplit")
      val pipeline = new StanfordCoreNLP (props)
      it flatMap { case (id, payload) =>
        val ann = new Annotation (payload)
        pipeline annotate ann
        (for (s <- ann get classOf[SentencesAnnotation]) yield (
          for (t <- s.get (classOf[TokensAnnotation]).map { x =>
            transform (x.originalText ())})
          yield t).toList).map { TokenizedRecord (id, _) }.toList
      }
    }
    maybeSavePath foreach ( tokenized.write.mode ("overwrite").option ("path", _).save )
    tokenized
  }

  /** Tokenize abstracts (after pattern replacement and conversion to lower case) 
    *  and save the result. */
  def saveTokenizedAbstracts (data: DataFrame, pathToSave: String) = {
    val abstracts = data.select ("eid", "abstr").as[(String, String)]
//      map {x => (x getString 0, x getString 1)}
    val abstractsReplace = replacement.replacePatterns (abstracts).map {x => (x._1, x._2.toLowerCase)}
    val tokens = tokenizePerSentence (abstractsReplace, None, None).map (_ terms)
    tokens.write.option ("path", pathToSave) save
  }

  /** Helper function. Given a dataset of sequences of tokens, and a ``per sequence`` aggregating function
    * computes the frequencies of each token in the dataset.
    * Definition of this frequency depends on the aggregating function.
    * @param data the dataset where to estimate the frequencies
    * @param seqop the aggregating function to apply to each individual sequence
    * @return a map `token => frequency` */
  private def extractVocabularyHelper (
    data: Dataset[Seq[String]], 
    seqop: (Map[String, Int], Seq[String]) => Map[String, Int]) = {
    val combop : (Map[String, Int], Map[String, Int]) => Map[String, Int] =
      (a, b) => b.foldLeft (a) { case (a, v) =>
        a + (v._1 -> (a.getOrElse (v._1, 0) + v._2))
      }

    data.rdd.aggregate (Map.empty[String, Int])(seqop, combop)
  }

  /** Count the nb of times each term is found
    * @param data the dataset where to estimate the frequencies
    * @return a map `token => frequency` */
  def extractVocabularyFrequencyForTokenized (data: Dataset[Seq[String]]) = 
    extractVocabularyHelper (
      data, 
      (a, xs) => xs.foldLeft (a) { (a, x) => a + (x -> (a.getOrElse (x, 0) + 1)) } )

  /** Count the number of documents countaining each terms (used in the ``document frequency`` of tfidf)
    * @param data the dataset where to estimate the frequencies
    * @return a map `token => frequency` */
  def extractVocabularyDocFrequencyForTokenized (data: Dataset[Seq[String]]) =
    extractVocabularyHelper (
      data, 
      (a, xs) => xs.toSet.foldLeft (a) { (a, x) => a + (x -> 1) } )

  /** Save vocabulary statistics computed on [[manuscripts]]. 
    * Both the abstracts and titles are used. 
    * @param maybeFrequencySavePath where to save raw term frequencies. 
    * raw term frequencies are the number of time a term (token) is seen, either in an abstract or a title.
    * @param maybeDocFrequencySavePath where to save term doc frequencies
    *   (nb of documents into which a term may be found, either in the abstract or the title)
    * @param maybeIdfWeightSavePath where to save idf weight for each term. 
    * idf is defined as `log (docfrequency/nbdocs)`
    * @return `(vocabularyFrequency, vocabularyDocFrequency)` maps */
  def extractVocabulary (
    maybeFrequencySavePath: Option[String] = None, 
    maybeDocFrequencySavePath: Option[String] = None, 
    maybeIdfWeightSavePath: Option[String]
  ) = {
    val vocabularyFrequencyAbstracts = 
      extractVocabularyFrequencyForTokenized (pr (path_abstractsTokenized).
        map (_ getSeq[String](1)))
    val vocabularyFrequencyTitles = 
      extractVocabularyFrequencyForTokenized (pr (path_titlesTokenized).
        map (_ getSeq[String](1)))
    val vocabularyFrequency = 
      vocabularyFrequencyAbstracts.
        foldLeft (vocabularyFrequencyTitles) { case (a, (term, freq)) =>
          a + (term -> (a.getOrElse (term, 0) + freq)) }
    maybeFrequencySavePath foreach {
      sc.parallelize (vocabularyFrequency.toSeq).toDF ("token", "freq").
        write.mode ("overwrite").option ("path", _) save }
    
    val vocabularyDocFrequencyAbstracts = 
      extractVocabularyDocFrequencyForTokenized (pr (path_abstractsTokenized).
        map (_ getSeq[String](1)))
    val vocabularyDocFrequencyTitles = 
      extractVocabularyDocFrequencyForTokenized (pr (path_titlesTokenized).
        map (_ getSeq[String](1)))
    val vocabularyDocFrequency = 
      vocabularyDocFrequencyAbstracts.
        foldLeft (vocabularyDocFrequencyTitles) { case (a, (term, _)) => a + (term -> 1) }
    maybeDocFrequencySavePath foreach {
      sc.parallelize (vocabularyDocFrequency.toSeq).toDF ("token", "freq").
        write.mode ("overwrite").option ("path", _) save }

    maybeIdfWeightSavePath foreach {
      val nDocs: Double = pr (path_abstractsTokenized) count
      val idfWeigths = vocabularyDocFrequency.
        map { case (w, n) => (w, Math.log (nDocs / n)) }.
        toSeq
      sc.parallelize (idfWeigths).toDF ("token", "weight").
        write.mode ("overwrite").option ("path", _) save
    }
    (vocabularyFrequency, vocabularyDocFrequency)
  }

  /** Computes a Word2Vec embedding from a dataset of sequences of tokens.
    * @param data 
    * @param embeddingSize dimension of the embedding
    * @param maybeSavePath where to save the embedding (a (String, Array[Float]) dataframe 
    * @return the Word2Vec model */
  def extractWordEmbedding_1 (
    data: Dataset[Seq[String]], 
    embeddingSize: Int, 
    maybeSavePath: Option[String]
  ) : Word2VecModel = {
    var model: Word2VecModel = null
    try {
      data cache
      val w2v = new Word2Vec ()
      w2v.setVectorSize (embeddingSize)
      model = w2v fit data.rdd
    }
    finally {
      data unpersist
    }
    
    maybeSavePath foreach { path =>
      session.sparkContext.
        parallelize (model.getVectors.toList).toDF.
        write.mode("overwrite").option ("path", path) save
    }
    model
  }

  /** Compute a Word2Vec embedding from a tokenized dataset. 
    * @param data 
    * @param embeddingSize dimension of the embedding
    * @param maybeSavePath where to save the embedding (a (String, Array[Float]) dataframe 
    * @return the Word2Vec model */
  def extractWordEmbedding (
    data: Dataset[TokenizedRecord], 
    embeddingSize: Int, 
    maybeSavePath: Option[String] = None
  ) : Word2VecModel =
    extractWordEmbedding_1 (data.map {_ terms}, embeddingSize, maybeSavePath)

  /** Creates a weighted Bag of Words from a tokenized dataset. 
    * Keys in the input dataset may contain duplicates. 
    * @param data
    * @param termWeights a map from terms to weight
    * @param maybeUnknownTermWeight the weight of a term that is not found in `termWeights` 
    * @param maybeSavePath wher to write the weighted dataset 
    * @return a dataset of [[WeightedTermRecord]] */
  def createWeighted (
    data: Dataset[TokenizedRecord], 
    termWeights: Map[String, Double],
    maybeUnknownTermWeight: Option[Double],
    maybeSavePath: Option[String]
  ) : Dataset[WeightedTermRecord] = {
    val unknownTermWeight = maybeUnknownTermWeight.getOrElse (
      termWeights.foldLeft (0.0) { case (a, (_, weight)) => Math.max (a, weight) } )
    val bTermWeights = sc.broadcast (termWeights)
    
    val ret = data.map { r => (r.id, r.terms) }.
      rdd.
      groupByKey.
      map { case (id, l) => (
        id,
        l.toSeq.flatMap { x => x }.
          foldLeft (Map.empty[String, Int]) { (a, t) => 
            a + (t -> (a.getOrElse (t, 0) + 1)) } )
      }.mapPartitions { it =>
        val termWeights: Map[String, Double] = bTermWeights.value        
        it map { case (id, localDic) =>
          val totalTermsInDoc: Double = 
            localDic.foldLeft (0) { case (a, (_, freq)) => a + freq }
          WeightedTermRecord (
            id,
            localDic.foldLeft (Map.empty[String, Double]) { case (a, (term, freq)) =>
              val tf = freq / totalTermsInDoc
              val idf = termWeights.getOrElse (term, unknownTermWeight)
              a + (term -> tf*idf)
            } ) } }.
      toDF.as[WeightedTermRecord]

    maybeSavePath foreach { ret.write.mode ("overwrite").option ("path", _) save }
    ret
  }

  /** Creates a weighted Bag of Words from a tokenized dataset. 
    * Keys in the input dataset may contain duplicates. 
    * @param data
    * @param termWeightsDataset a dataset of (terms, weight) pairs
    * @param maybeUnknownTermWeight the weight of a term that is not found in `termWeights` 
    * @param maybeSavePath wher to write the weighted dataset 
    * @return a dataset of [[WeightedTermRecord]] */
  def createWeighted (
    data: Dataset[TokenizedRecord], 
    termsWeightsDataset: Dataset[(String, Double)], 
    maybeUnknownTermWeight: Option[Double],
    maybeSavePath: Option[String]
  ) : Dataset[WeightedTermRecord] =
    createWeighted (data, termsWeightsDataset.collect.toMap, maybeUnknownTermWeight, maybeSavePath)

  /** Creates a weighted Bag of Words from a tokenized dataset. 
    * Keys in the input dataset may contain duplicates. 
    * @param data
    * @param termWeightsPath a path to a dataset of (terms, weight) pairs (default: [[path_tokensIdfWeights]])
    * @param maybeUnknownTermWeight the weight of a term that is not found in `termWeights` 
    * @param maybeSavePath wher to write the weighted dataset 
    * @return a dataset of [[WeightedTermRecord]] */
  def createWeighted (
    data: Dataset[TokenizedRecord], 
    termsWeightsPath: String = path_tokensIdfWeights, 
    maybeUnknownTermWeight: Option[Double] = None,
    maybeSavePath: Option[String] = None
  ) : Dataset[WeightedTermRecord] = 
    createWeighted (data, pr (termsWeightsPath).as[(String, Double)], maybeUnknownTermWeight, maybeSavePath)

  /** Compute the embedding of manuscripts as the weighted average of the embeddings of its terms.
    * @param weightedBoWData the weighted  terms of the manuscripts
    * @param wordEmbeddingData the word embeddings
    * @param maybeSavePath where to write the new dataset
    * @return the new dataset */
  def computeManuscriptsEmbeddings (
    weightedBoWData: Dataset[WeightedTermRecord], 
    wordEmbeddingData: Dataset[(Term, Seq[Float])], 
    maybeSavePath: Option[String] = None
  ) : Dataset[EmbeddingRecord[Double]]= {
    val weightedBoWFlattenData = 
      weightedBoWData.flatMap { r =>
        r.weights.toSeq.map { case (term, weight) => (term, (r.id, weight)) }
      }.toDF ("term", "payload")
    
    val weightsEmbeddingJoinData = 
      wordEmbeddingData.toDF ("term", "embedding").
        join (weightedBoWFlattenData, "term").
        select ("embedding", "payload").
        as[(Seq[Float], (ManuscriptId, Double))].
        map { case (termEmbedding, (manuscriptId, termWeight)) =>
          val weightedEmbedding = for (e <- termEmbedding) yield e*termWeight
        (manuscriptId, (termWeight, for (e <- termEmbedding) yield e*termWeight))
      }

    // Aggregate each term embeddings
    val tfidf_embedding_agg_ds = weightsEmbeddingJoinData.rdd.reduceByKey { (a, b) =>
      (a._1 + b._1, (a._2 zip b._2) map { x => x._1 + x._2 })
    }

    // Normalize
    val manuscriptsEmbedding = 
      tfidf_embedding_agg_ds.mapValues { case (n, xs) => xs.map { _ / n } }.
        toDF ("id", "embedding").as[EmbeddingRecord[Double]]
    
    maybeSavePath foreach { manuscriptsEmbedding.write.mode("overwrite").option ("path", _) save }
    
    manuscriptsEmbedding
  }

  /** Compute the embedding of a sequence of terms as a weighted average of the embeddings of the terms */
  def computeTokensSeqEmbedding (
    tokens: Seq[String], 
    wordEmbeddingData: Dataset[WordEmbeddingRecord], 
    weightMap: Map[String, Double]
  ) : Array[Double] = {
    val tokenCount: Double = tokens size
    val tokenFrequencies = 
      tokens.foldLeft (tokens.map { (_, 0) }.toMap) { (a, t) => a + (t -> (a.getOrElse (t, 0) + 1)) }.toSeq
    val tokenWeights = tokenFrequencies.map { case (w, f) =>
      val tokenWeight: Double = weightMap.getOrElse (w, 0.0)
      (w, tokenWeight * (f / tokenCount))
    }
    val unnormalizedEmbedding =
      sc.parallelize (tokenWeights.toSeq).
        toDF ("word", "weight").
        join (wordEmbeddingData, "word").
        collect.
        map { r => (r getDouble 1, r getSeq[Float] 2) }.
        foldLeft ((0.0, Array.empty[Float])) { (a, x) =>
          (   a._1 + x._1,
            if (a._2.isEmpty) x._2.toArray
            else a._2.zip (x._2).map {y => y._1 + y._2 }
          ) }
    unnormalizedEmbedding._2.map { _ / unnormalizedEmbedding._1 }
  }

  /** computes the embedding of a sequence of texts as the weighted average of their terms. 
    * Tokenization is the simpler one (no transformation nor pattern replacement 
    * @param texts
    * @param wordEmbeddingData
    * @param weightData
    * @return a sequence of embedding */
  def computeTextEmbedding (
    texts: Seq[String], 
    wordEmbeddingData: Dataset[WordEmbeddingRecord], 
    weightData: Dataset[(String, Double)]
  ) : Seq[Array[Double]] =
    computeTextEmbedding (texts, wordEmbeddingData, weightData.collect.toMap)
  
  /** computes the embedding of a sequence of texts as the weighted average of their terms. 
    * Tokenization is the simpler one (no transformation nor pattern replacement 
    * @param texts
    * @param wordEmbeddingData
    * @param weightMap
    * @return a sequence of embedding */
  def computeTextEmbedding (
    texts: Seq[String], 
    wordEmbeddingData: Dataset[WordEmbeddingRecord], 
    weightMap: Map[String, Double]
  ) : Seq[Array[Double]] =
    tokenizeTextPerSentence (texts) map { ts => computeTokensSeqEmbedding (ts, wordEmbeddingData, weightMap) }

  /** creates a One Hot Encoding data to train classifier on areas.
    * @param manuscriptsEmbeddingPath 
    * @param maybeSavePath where to write OHE data
    * @param maybeAreaIndexSavePath where to write the data informing on 
    * the position of each area in the OHE vector
    * @return a pair of these two data sets */
  def createOneHotAreaEncodingForClassification (
    manuscriptsEmbeddingPath: String, 
    maybeSavePath: Option[String] = None, 
    maybeAreaIndexSavePath: Option[String] = None
  ) : (Dataset[OHETrainingDataRecord], Dataset[(String, Int)]) = {
    val manuscriptsEmbedding = pr (manuscriptsEmbeddingPath).toDF ("eid", "embedding")
    val areaToIndexMap : Map[String, Int] = flattenSubjAreas.
      select ("area").
      map { _ getString 0 }.
      distinct.
      collect.
      sorted.
      zipWithIndex.
      toMap

    val areaToIndexDS = sc.parallelize (areaToIndexMap.toSeq).toDF ("area", "areaIndex")
    val flattenSubjAreasIndex = 
      flattenSubjAreas.join (areaToIndexDS, "area").select ("eid", "areaIndex")
    val OneHotEnc = 
      flattenSubjAreasIndex.as[(String, Int)].rdd.
        aggregateByKey (Array.fill[Int](numberOfAreas)(0)) (
          (a, i) => { a (i) = 1; a },
          (a, b) => (a zip b) map { case (x, y) => x + y } ).
        toDF("eid", "hotarea").
        join (manuscriptsEmbedding, "eid").
        toDF ("id", "ohe", "features").
        as[OHETrainingDataRecord]
    
    maybeSavePath foreach { OneHotEnc.write.mode ("overwrite").option ("path", _) save }
    maybeAreaIndexSavePath foreach { areaToIndexDS.write.mode ("overwrite").option ("path", _) save }
    (OneHotEnc, areaToIndexDS.as[(String, Int)])
  }

  /** Computes a logistic regression classifier to predict one area, 
    * identified by its index.
    * @param areaIndex
    * @param OHEData
    * @param trainProportion split train/test 
    * @return a pair of the correct predictions and testing data set */ 
  def areaLRModel (
    areaIndex: Int, 
    OHEData: Dataset[OHETrainingDataRecord],
    trainProportion: Double
  ) : (Long, Long) = {
    val labeledPointsData = 
      OHEData.map { x =>
          LabeledPoint (x.ohe(areaIndex), Vectors dense (x.features toArray)) }

    val Array(training, testing) = 
      labeledPointsData.randomSplit (Array (trainProportion, 1.0 - trainProportion))
    val lr = new LogisticRegression ()
    val lrModel = lr.fit (training)
    (lrModel.transform (testing).filter {x => x.getDouble (0) == x.getDouble(4)}.count,
      testing count)
  }

  /** Computes a logistic regression classifier to predict one area, 
    * identified by its index.
    * @param areaIndex
    * @param OHEPath the path to the OHE dataset
    * @param trainProportion split train/test (default: 0.8 => train on 80% of the DS)
    * @return a pair of the correct predictions and testing data set */ 
  def areaLRModel (
    areaIndex: Int, 
    OHEPath: String = path_areaOneHotEncodingForClassification, 
    trainProportion: Double=0.8
  ): (Long, Long) =
    areaLRModel (areaIndex, pr (OHEPath).as[OHETrainingDataRecord], trainProportion)

  /** Computes a logistic regression model for all area.
    * @param OHEData the data to train on and test on
    * @param maybeAreaOHEIndexPath: the path to the area => index file, used
    * to fill the result with the name (otherwise the index is used)
    * @param trainProportion
    * @param maybePerformancesSavePath
    * @return the performances as a dataset */
  def allAreaOHELRModel (
    OHEData: Dataset[OHETrainingDataRecord],
    maybeAreaOHEIndexPath: Option[String], 
    trainProportion: Double, 
    maybePerformancesSavePath: Option[String]
  ) : Dataset[OHEClassificationModelPerformancesRecord] = {
    val modelPerformances = 
      (0 until numberOfAreas).
        foldLeft (List.empty[(Int, Long, Long, Double)]) { (a, i) =>
          val (fitCount, testCount) = areaLRModel (i, OHEData, trainProportion)
          a :+ (i, fitCount, testCount, (100.0*fitCount) / testCount) }

    val modelPerformancesDF = sc.parallelize (modelPerformances).
      toDF ("areaIndex", "fitcount", "totalcount", "accuracy")
    val modelPerformancesWithLabelsDF =
      maybeAreaOHEIndexPath.
        map { path => pr (path).join (modelPerformancesDF, "areaIndex") }.
        getOrElse (modelPerformancesDF.withColumn ("label", $"areaIndex" cast StringType)).
        select ("label", "fitcount", "totalcount", "accuracy")
    maybePerformancesSavePath foreach { modelPerformancesWithLabelsDF.write.mode ("overwrite").option ("path", _) save }
    modelPerformancesWithLabelsDF.as[OHEClassificationModelPerformancesRecord]
  }

  /** Computes a logistic regression model for all area.
    * @param OHEPath the data to train on and test on
    * @param maybeAreaOHEIndexPath: the path to the area => index file, used
    * to fill the result with the name (otherwise the index is used)
    * @param trainProportion
    * @param maybePerformancesSavePath
    * @return the performances as a dataset */
  def allAreaOHELRModel (
    OHEPath: String = path_areaOneHotEncodingForClassification, 
    maybeAreaOHEIndexPath: Option[String] = None, 
    trainProportion: Double=0.8, 
    maybePerformancesSavePath: Option[String] = None
  ) : Dataset[OHEClassificationModelPerformancesRecord] = 
    allAreaOHELRModel (pr (OHEPath).as[OHETrainingDataRecord],
      maybeAreaOHEIndexPath, trainProportion, maybePerformancesSavePath)

  /** For a given set of areas, creates an OHE dataset */
  def createISSN_OHEDataset (
    areas: Seq[String], 
    maybeSavePath: Option[String] = None) = {
    // ids of manuscripts for these areas
    val eids = 
      flattenSubjAreas.
        where ($"area".isin (areas: _*)).
        select ("eid").
        distinct
    // table (issn, index in the OHE vector)
    val issns = manuscripts.
      select ("eid", "issn").
      join (eids, "eid").
      groupBy ("issn").
      count.
      orderBy ($"count" desc).
      select ("issn").
      map {_ getString 0}.
      collect.
      zipWithIndex
    val issnDF = sc.parallelize (issns).toDF ("issn", "index")
    val N = issns.size
    val ret = manuscripts.
      select ("eid", "issn").
      join (issnDF, "issn").
      map { x =>
        val eid = x getString 1
        val index = x getInt 2
        val oheArray = Array.fill[Int](N)(0)
        oheArray (index) = 1
        (eid, oheArray)
      }.
      toDF ("id", "ohe")
    maybeSavePath foreach { ret.write.mode ("overwrite").option ("path", _) save }
    ret
  }

  def doitSaveTokenizedAbstracts =
    saveTokenizedAbstracts (manuscripts, "s3://wads/epfl/thy/manuscripts-content-tokenized-abstracts")

  def doitSaveTokenizePerSentence = {
    tokenizePerSentence (
      manuscripts.
        select ("eid", "abstr").
        map {x => (x getString 0, x getString 1)},
      None, Some (path_abstractsTokenized))

    tokenizePerSentence (
      manuscripts.
        select ("eid", "title").
        map {x => (x getString 0, x getString 1)},
      None, Some (path_titlesTokenized))
  }
  
  def doitExtractWordsEmbedding =
    extractWordEmbedding (session.read.parquet (path_abstractsTokenized).as[TokenizedRecord], 
      100, 
      Some (path_wordEmbeddingOnAbstracts))

  def doitExtractVocabulary =
    extractVocabulary (Some (path_tokensTotalFrequencies), Some (path_tokensDocFrequencies), Some (path_tokensIdfWeights))

  def doitSaveTFIDFWeighted =
    createWeighted (pr (path_abstractsTokenized).as[TokenizedRecord], maybeSavePath = Some (path_tfidfWeights))
 
  def doitExtractManuscriptsEmbedding = {
    val tfidf_ds = pr (path_tfidfWeights).as[WeightedTermRecord]
    val wordEmbedding_ds = pr (path_wordEmbeddingOnAbstracts).as[(String, Seq[Float])]
    computeManuscriptsEmbeddings (tfidf_ds, wordEmbedding_ds, Some(path_manuscriptsEmbedding))
  }

  def doitExtractAreaOHE4Classif =
    createOneHotAreaEncodingForClassification (
      path_manuscriptsEmbedding, 
      Some (path_areaOneHotEncodingForClassification), 
      Some (path_areaOneHotEncodingIndex))
  
  def doitAllAreaOHELRModels =
    allAreaOHELRModel (
      path_areaOneHotEncodingForClassification, 
      Some (path_areaOneHotEncodingIndex), 
      0.8, 
      Some (path_areaOneHotEncodingModelPerformances))
}

/** Used to compute model predicting issns, restricted to some area */ 
class ISSNClassifyOneAgainstOthers (areas: Seq[String], app: EmbeddingApp) (implicit session: SparkSession) {
  import EmbeddingApp._
  import session.implicits._

  val embedding = session.read.parquet (path_manuscriptsEmbedding)
  val eids = app.flattenSubjAreas.
    where ($"area" isin (areas:_*)).
    join (app.manuscripts, "eid").
    select ($"eid" as "id", $"issn")

  /** given a list of issn, train the first issn against the others.
    * @param issns,
    * @param trainProportion
    * @return a 4-uple: (# of fits on the test, size of the test, # of fits on the complement, size of the complement) */
  def classify (issns: Seq[String], trainProportion: Double = 0.8) = {
    val learn_eids = 
      app.manuscripts.
        where ($"issn" isin (issns:_*)).
        select ($"eid" as "id", $"issn")
    val complement = 
      eids.
        where (! ($"issn" isin (issns:_*))).
        join (embedding, "id").
        map { x =>
          val features = Vectors dense (x.getSeq[Double] (2).toArray)
          LabeledPoint (0.0, features)
        }
    val learn_data = 
      learn_eids.
        join (embedding, "id").
        map { x =>
          val issn = x getString 1
          val label = if (issn == issns(0)) 1.0 else 0.0
          val features = Vectors dense (x.getSeq[Double] (2).toArray)
          LabeledPoint (label, features)
        }
    val Array(training, testing) = learn_data.randomSplit (Array (trainProportion, 1.0 - trainProportion))
    val lr = new LogisticRegression ()
    val lrModel = lr.fit (training)
    (lrModel.transform (testing).filter {x => x.getDouble (0) == x.getDouble(4)}.count, testing count,
      lrModel.transform (complement).filter {x => x.getDouble (0) == x.getDouble(4)}.count, complement count)
  }

  def getIssnFreq (areas: Seq[String]) =
    app.manuscripts.
      select ("eid", "issn").
      join (app.flattenSubjAreas.where ($"area" isin (areas:_*)), "eid").
      groupBy ("issn").
      count.
      orderBy ($"count" desc)
}

/** a demo of [[ISSNClassifyOneAgainstOthers$]] */
object ISSNClassifyOneAgainstOthers {
    def demo (implicit session: SparkSession) = {
      val app = new EmbeddingApp (ElsStdEmbeddingConfig)
      val x = new ISSNClassifyOneAgainstOthers (List("MATE"), app)
      x.classify (List ("09258388","01694332", "19448244", "20507488"))
    }
}

